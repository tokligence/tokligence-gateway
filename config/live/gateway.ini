# Overrides for the live environment
#
# Configuration Hierarchy (priority from high to low):
# 1. Environment variables (e.g., TOKLIGENCE_WORK_MODE, TOKLIGENCE_OPENAI_API_KEY)
#    - Highest priority, overrides everything
#    - Set via: export TOKLIGENCE_XXX=value or in .env file
# 2. This gateway.ini file (environment-specific: live/dev/test)
#    - Environment-specific overrides
#    - Loaded based on TOKLIGENCE_ENV or --env flag
# 3. Default values in code
#    - Lowest priority, used when no override is specified
#
# Example: If TOKLIGENCE_WORK_MODE=passthrough is set as env var,
#          it will override work_mode=auto in this file
#
base_url=https://marketplace.tokligence.ai
# Port scheme: facade=8081, admin=8079, openai=8082, anthropic=8083
facade_port=8081
ledger_path=~/.tokligence/ledger-live.db
publish_name=local-live
model_family=claude-3.5-sonnet
price_per_1k=1.0000
auth_secret=tokligence-live-secret

# Work Mode: controls passthrough vs translation behavior globally for all endpoints
# This is a critical setting that affects how the gateway handles requests:
#   - auto (default):        Smart routing - automatically choose passthrough or translation
#                            based on endpoint+model match (e.g., /v1/responses+gpt* = passthrough,
#                            /v1/responses+claude* = translation)
#   - passthrough:           Delegation-only mode - only allow direct passthrough/delegation
#                            to upstream providers, reject any translation requests
#   - translation:           Translation-only mode - only allow translation between API formats,
#                            reject any passthrough requests
# Override with: TOKLIGENCE_WORK_MODE=auto|passthrough|translation
work_mode=auto

# Model-first provider routing (pattern=>provider). Auto mode uses these rules to pick
# the upstream provider based on the requested model, regardless of which endpoint was used.
# Adjust this list if you add providers (e.g., kimi*, qwen*). Commas or newlines are allowed.
# Example: model_provider_routes=gpt*=openai,claude*=anthropic
model_provider_routes=gpt*=openai,claude*=anthropic
