# Overrides for the dev environment
#
# Configuration Hierarchy (priority from high to low):
# 1. Environment variables (e.g., TOKLIGENCE_WORK_MODE, TOKLIGENCE_OPENAI_API_KEY)
#    - Highest priority, overrides everything
#    - Set via: export TOKLIGENCE_XXX=value or in .env file
# 2. This gateway.ini file (environment-specific: live/dev/test)
#    - Environment-specific overrides
#    - Loaded based on TOKLIGENCE_ENV or --env flag
# 3. Default values in code
#    - Lowest priority, used when no override is specified
#
# Example: If TOKLIGENCE_WORK_MODE=passthrough is set as env var,
#          it will override work_mode=auto in this file
#
# Token Exchange endpoint for dev workflows - using tokligence domain
base_url=https://dev.tokligence.ai
# Dev account identity; typically a test inbox
email=dev@example.com
# Name shown when listing the gateway locally
display_name=Dev Gateway
# Toggle provider publishing for local testing
enable_provider=false
ledger_path=~/.tokligence/ledger-dev.db
# Service name to push to marketplace during dev
publish_name=local-dev
# Label for the model class advertised to consumers
model_family=claude-3.5-sonnet
price_per_1k=0.5000
auth_secret=tokligence-dev-secret
# Dev-specific log files
# Override per process if needed:
#   TOKLIGENCE_LOG_FILE_CLI=... TOKLIGENCE_LOG_FILE_DAEMON=...
log_file_cli=logs/dev-cli.log
log_file_daemon=logs/dev-gatewayd.log
log_level=debug

# Bridge session management (deprecated; stateless bridge path in use)
bridge_session_enabled = false
bridge_session_ttl = 5m
bridge_session_max_count = 1000
# Disable auth for dev convenience
auth_disabled=true

# Model alias hot-reload sources (optional)
# Directory of alias files where each line is "incoming=>target" or "incoming=target"
model_aliases_dir=config/model_aliases.d

# Provider API configuration (placeholders)
# openai_api_key=
# openai_base_url=
# openai_org=
# anthropic_api_key=
# anthropic_base_url=
# anthropic_version=

# Work Mode: controls passthrough vs translation behavior globally for all endpoints
# This is a critical setting that affects how the gateway handles requests:
#   - auto (default):        Smart routing - automatically choose passthrough or translation
#                            based on endpoint+model match (e.g., /v1/responses+gpt* = passthrough,
#                            /v1/responses+claude* = translation)
#   - passthrough:           Delegation-only mode - only allow direct passthrough/delegation
#                            to upstream providers, reject any translation requests
#   - translation:           Translation-only mode - only allow translation between API formats,
#                            reject any passthrough requests
# Override with: TOKLIGENCE_WORK_MODE=auto|passthrough|translation
work_mode=auto

# Model-first provider routing (pattern=>provider). The gateway inspects the requested
# model and picks the provider before considering which endpoint was called. Customize
# this list to add providers such as kimi*/qwen*/etc. Accepted separators: commas or newlines.
# Examples:
#   model_provider_routes=gpt*=openai,claude*=anthropic
#   model_provider_routes=o1*=openai,glm*=kimi
model_provider_routes=gpt*=openai,claude*=anthropic

# Facade / multi-port configuration (default: single-port mode on :8081)
# Port scheme: facade=8081, admin=8079, openai=8082, anthropic=8083
enable_facade=true
multiport_mode=true
facade_port=8081
admin_port=8079
openai_port=8082
anthropic_port=8083
# Optional endpoint selections per port (defaults: facade=openai_core,openai_responses,anthropic,admin,health)
# facade_endpoints=
# admin_endpoints=
# openai_endpoints=
# anthropic_endpoints=

# Anthropic beta capability toggles (off by default)
#anthropic_web_search=false
#anthropic_computer_use=false
#anthropic_mcp=false
#anthropic_prompt_caching=false
#anthropic_json_mode=false
#anthropic_reasoning=false

# ==============================================================================
# Prompt Firewall Configuration
# ==============================================================================
# The firewall protects against PII leakage and sensitive data exposure
# in both user prompts (input) and LLM responses (output).
#
# Modes:
#   - redact:   Detect PII → Replace with tokens → Restore in responses (DEFAULT, recommended for production)
#   - monitor:  Log violations but allow requests through
#   - enforce:  Block requests that violate policies
#   - disabled: Firewall completely disabled
#
# Override with: TOKLIGENCE_FIREWALL_MODE=redact|monitor|enforce|disabled

[firewall]
enabled = true
mode = redact

# PII patterns configuration file
pii_patterns_file = config/pii_patterns.ini

# Enabled regions (comma-separated)
# Available: global, us, cn, eu, uk, ca, au, in, jp, de, fr, sg
# Leave empty to use defaults from pii_patterns.ini
pii_regions = global,us,cn

# Alternatively, specify exact PII types (comma-separated, overrides regions if set)
# Available types: EMAIL, PHONE, SSN, CREDIT_CARD, IP_ADDRESS, API_KEY, CRYPTO_ADDRESS,
#                  NATIONAL_ID, PASSPORT, DRIVERS_LICENSE, BANK_ACCOUNT, TAX_ID, MEDICAL_ID, etc.
# pii_types = EMAIL,PHONE,SSN,NATIONAL_ID,API_KEY

# Minimum confidence threshold (0.0-1.0)
# Only detections above this threshold will trigger actions
pii_min_confidence = 0.75

# Log all firewall decisions (useful for debugging)
log_decisions = true

# Log PII values in debug mode (WARNING: disable in production for security)
log_pii_values = false

# Maximum PII entities in a single request (soft limit, logs warning if exceeded)
max_pii_entities = 50

# ==============================================================================
# PII Tokenizer Configuration (for redact mode)
# ==============================================================================
[tokenizer]
# Storage backend: memory | redis | redis_cluster
# - memory: Fast, single-instance, lost on restart (default for dev)
# - redis: Persistent, supports multiple gateway instances
# - redis_cluster: High availability, distributed
store_type = memory

# Time-to-live for token mappings (duration format: 1h, 30m, etc.)
ttl = 1h

# Redis configuration (only used if store_type = redis)
# redis_addr = localhost:6379
# redis_password =
# redis_db = 0
# redis_key_prefix = firewall:tokens

# Redis Cluster configuration (only used if store_type = redis_cluster)
# redis_cluster_addrs = redis-node1:7000,redis-node2:7001,redis-node3:7002
# redis_cluster_key_prefix = firewall:tokens

# ==============================================================================
# Input Filters (applied to user prompts before sending to LLM)
# ==============================================================================
[firewall_input_filters]
# Built-in PII regex filter (fast, no external dependencies)
filter_pii_regex_enabled = true
filter_pii_regex_priority = 10

# External Presidio service (optional, higher accuracy)
# Uncomment to enable after starting Presidio sidecar
# filter_presidio_enabled = false
# filter_presidio_priority = 20
# filter_presidio_endpoint = http://localhost:7317/v1/filter/input
# filter_presidio_timeout_ms = 500
# filter_presidio_on_error = allow

# ==============================================================================
# Output Filters (applied to LLM responses before returning to user)
# ==============================================================================
[firewall_output_filters]
# Built-in PII regex filter for detokenization
filter_pii_regex_enabled = true
filter_pii_regex_priority = 10

# Presidio output filter (optional)
# filter_presidio_enabled = false
# filter_presidio_priority = 20
# filter_presidio_endpoint = http://localhost:7317/v1/filter/output
# filter_presidio_timeout_ms = 500
# filter_presidio_on_error = allow
