# Overrides for the dev environment
#
# Configuration Hierarchy (priority from high to low):
# 1. Environment variables (e.g., TOKLIGENCE_WORK_MODE, TOKLIGENCE_OPENAI_API_KEY)
#    - Highest priority, overrides everything
#    - Set via: export TOKLIGENCE_XXX=value or in .env file
# 2. This gateway.ini file (environment-specific: live/dev/test)
#    - Environment-specific overrides
#    - Loaded based on TOKLIGENCE_ENV or --env flag
# 3. Default values in code
#    - Lowest priority, used when no override is specified
#
# Example: If TOKLIGENCE_WORK_MODE=passthrough is set as env var,
#          it will override work_mode=auto in this file
#
# Token Exchange endpoint for dev workflows - using tokligence domain
base_url=https://dev.tokligence.ai
# Dev account identity; typically a test inbox
email=dev@example.com
# Name shown when listing the gateway locally
display_name=Dev Gateway
# Toggle provider publishing for local testing
enable_provider=false
ledger_path=~/.tokligence/ledger-dev.db
# Service name to push to marketplace during dev
publish_name=local-dev
# Label for the model class advertised to consumers
model_family=claude-3.5-sonnet
price_per_1k=0.5000
auth_secret=tokligence-dev-secret
# Dev-specific log files
# Override per process if needed:
#   TOKLIGENCE_LOG_FILE_CLI=... TOKLIGENCE_LOG_FILE_DAEMON=...
log_file_cli=logs/dev-cli.log
log_file_daemon=logs/dev-gatewayd.log
log_level=debug

# Bridge session management (deprecated; stateless bridge path in use)
bridge_session_enabled = false
bridge_session_ttl = 5m
bridge_session_max_count = 1000
# Disable auth for dev convenience
auth_disabled=true

# Model alias hot-reload sources (optional)
# Directory of alias files where each line is "incoming=>target" or "incoming=target"
model_aliases_dir=config/model_aliases.d

# Provider API configuration (placeholders)
# openai_api_key=
# openai_base_url=
# openai_org=
# anthropic_api_key=
# anthropic_base_url=
# anthropic_version=

# Work Mode: controls passthrough vs translation behavior globally for all endpoints
# This is a critical setting that affects how the gateway handles requests:
#   - auto (default):        Smart routing - automatically choose passthrough or translation
#                            based on endpoint+model match (e.g., /v1/responses+gpt* = passthrough,
#                            /v1/responses+claude* = translation)
#   - passthrough:           Delegation-only mode - only allow direct passthrough/delegation
#                            to upstream providers, reject any translation requests
#   - translation:           Translation-only mode - only allow translation between API formats,
#                            reject any passthrough requests
# Override with: TOKLIGENCE_WORK_MODE=auto|passthrough|translation
work_mode=auto

# Model-first provider routing (pattern=>provider). The gateway inspects the requested
# model and picks the provider before considering which endpoint was called. Customize
# this list to add providers such as kimi*/qwen*/etc. Accepted separators: commas or newlines.
# Examples:
#   model_provider_routes=gpt*=openai,claude*=anthropic
#   model_provider_routes=o1*=openai,glm*=kimi
model_provider_routes=gpt*=openai,claude*=anthropic

# Facade / multi-port configuration (default: single-port mode on :8081)
# Port scheme: facade=8081, admin=8079, openai=8082, anthropic=8083
enable_facade=true
multiport_mode=true
facade_port=8081
admin_port=8079
openai_port=8082
anthropic_port=8083
# Optional endpoint selections per port (defaults: facade=openai_core,openai_responses,anthropic,admin,health)
# facade_endpoints=
# admin_endpoints=
# openai_endpoints=
# anthropic_endpoints=

# Anthropic beta capability toggles (off by default)
#anthropic_web_search=false
#anthropic_computer_use=false
#anthropic_mcp=false
#anthropic_prompt_caching=false
#anthropic_json_mode=false
#anthropic_reasoning=false
