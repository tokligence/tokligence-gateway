# Tokligence Gateway - Priority-Based Scheduler Configuration
#
# This configuration file controls the request scheduling system for gateway operators
# who are selling self-hosted LLM token throughput to multiple tenants/consumers.
#
# ============================================================================
# Configuration Hierarchy (priority from high to low):
# ============================================================================
# 1. Environment variables (e.g., TOKLIGENCE_SCHEDULER_ENABLED)
#    - Highest priority, overrides everything
#    - Set via: export TOKLIGENCE_SCHEDULER_XXX=value
# 2. This scheduler.ini file
#    - Scheduler-specific configuration
# 3. Default values in code
#    - Lowest priority
#
# Example:
#   scheduler.ini has scheduler_enabled=true
#   → TOKLIGENCE_SCHEDULER_ENABLED=false is set
#   → Final value: false (env var wins)
# ============================================================================

# ============================================================================
# Enable/Disable Scheduler
# ============================================================================
# Enable priority-based scheduler (default: false for backward compatibility)
# When enabled, requests are queued and scheduled based on priority levels
#
# Use Cases:
#   - Multi-tenant gateways selling LLM capacity
#   - Preventing production interference from dev/test traffic
#   - SLA-differentiated pricing (Gold/Silver/Bronze tiers)
#   - Fair resource allocation across teams/projects
#
# Override: TOKLIGENCE_SCHEDULER_ENABLED=true|false
scheduler_enabled = false

# ============================================================================
# Priority Queue Configuration
# ============================================================================
# Number of priority buckets (default: 10, supported range: 5-20)
#
# Priority Level Mapping (default 10-level system):
#   P0 = PriorityCritical    - Internal critical services, health checks
#   P1 = PriorityUrgent      - High-value urgent requests, VIP emergencies
#   P2 = PriorityHigh        - Partner/premium users, enterprise tier
#   P3 = PriorityElevated    - Elevated priority, business tier
#   P4 = PriorityAboveNormal - Above normal, pro tier
#   P5 = PriorityNormal      - Standard users, free tier (DEFAULT)
#   P6 = PriorityBelowNormal - Below normal, rate-limited users
#   P7 = PriorityLow         - Low priority, batch API
#   P8 = PriorityBulk        - Bulk/batch processing, data exports
#   P9 = PriorityBackground  - Background jobs, cleanup, analytics
#
# Override: TOKLIGENCE_SCHEDULER_PRIORITY_LEVELS=10
scheduler_priority_levels = 10

# Default priority for requests without explicit X-Priority header
# (default: 5 = PriorityNormal for 10-level system)
#
# How to set priority in requests:
#   curl -H "X-Priority: 0" ...  # Critical priority
#   curl -H "X-Priority: 2" ...  # High priority
#   curl -H "X-Priority: 5" ...  # Normal priority (default)
#
# Override: TOKLIGENCE_SCHEDULER_DEFAULT_PRIORITY=5
scheduler_default_priority = 5

# ============================================================================
# Scheduling Policy
# ============================================================================
# Scheduling algorithm (default: hybrid)
#
# Options:
#   - strict: Strict priority - Always serve highest priority first
#             Pros: Guarantees critical request latency
#             Cons: May starve low-priority queues under heavy load
#
#   - wfq: Weighted Fair Queuing - Fairness with weights
#          Pros: Prevents starvation, fair bandwidth sharing
#          Cons: Slightly higher latency for critical requests
#
#   - hybrid: Hybrid (RECOMMENDED) - P0 strict, P1-P9 WFQ
#             Pros: Best of both worlds - guarantees critical, fair for others
#             Cons: Slightly more complex
#
# Override: TOKLIGENCE_SCHEDULER_POLICY=strict|wfq|hybrid
scheduler_policy = hybrid

# Max depth per priority queue (default: 5000)
# When queue is full, new requests are rejected with 503 Service Unavailable
#
# Tuning:
#   - Higher values = More buffering, longer wait times, better burst handling
#   - Lower values = Faster rejection under overload
#   - Recommended: 5000-10000 for production (channel-based scheduler)
#
# Note: With channel-based scheduler, higher values are recommended since
# channels are memory-efficient and don't suffer from lock contention
#
# Override: TOKLIGENCE_SCHEDULER_MAX_QUEUE_DEPTH=5000
scheduler_max_queue_depth = 5000

# Queue timeout in seconds (default: 30)
# Requests waiting in queue longer than this will be expired and rejected with 503
#
# Per-priority timeout (optional, future):
#   P0: 5s (critical needs fast feedback)
#   P5: 30s (normal can wait)
#   P9: 60s (background can wait longer)
#
# Override: TOKLIGENCE_SCHEDULER_QUEUE_TIMEOUT_SEC=30
scheduler_queue_timeout_sec = 30

# ============================================================================
# Capacity Limits (Multi-Dimensional)
# ============================================================================
# These limits define the maximum capacity of your self-hosted LLM cluster.
# Scheduler uses these to decide when to queue vs. execute requests.
#
# PRIMARY METRIC: Tokens/sec (most important for LLM workloads)
# -----------------------------------------------------------------------
# Maximum tokens per second your cluster can process
# (default: 10000 tokens/sec)
#
# How to determine:
#   1. Benchmark your LLM: tokligence benchmark --profile long_context
#   2. Set to 80% of max throughput for safety margin
#   3. Example: If benchmark shows 12,500 tokens/sec, set to 10,000
#
# Override: TOKLIGENCE_SCHEDULER_MAX_TOKENS_PER_SEC=10000
scheduler_max_tokens_per_sec = 10000

# SECONDARY METRICS (fallback if tokens/sec unknown)
# -----------------------------------------------------------------------
# Maximum requests per second (default: 100)
# Used when token count estimation is unavailable
#
# Override: TOKLIGENCE_SCHEDULER_MAX_RPS=100
scheduler_max_rps = 100

# Maximum concurrent requests (default: 100)
# Prevents overloading LLM with too many parallel requests
#
# Tuning:
#   - GPU memory limited: Lower this (e.g., 50)
#   - Many small requests: Higher this (e.g., 200)
#
# Override: TOKLIGENCE_SCHEDULER_MAX_CONCURRENT=100
scheduler_max_concurrent = 100

# Maximum context window length (default: 128000 tokens)
# Requests exceeding this will be rejected immediately
#
# Model-specific limits:
#   - GPT-4 Turbo: 128,000
#   - Claude 3.5 Sonnet: 200,000
#   - Llama 3 70B: 8,192 (adjust accordingly)
#
# Override: TOKLIGENCE_SCHEDULER_MAX_CONTEXT_LENGTH=128000
scheduler_max_context_length = 128000

# ============================================================================
# Monitoring and Observability
# ============================================================================
# Stats logging interval in seconds (default: 180 = 3 minutes)
# Set to 0 to disable periodic stats logging
#
# Recommendations:
#   - Development/debugging: 30-60 seconds
#   - Production: 180-300 seconds (3-5 minutes)
#   - Disable if using external monitoring: 0
#
# Note: Stats are only logged when there is activity (queued requests or
# scheduled requests). Idle scheduler will not log.
#
# Override: TOKLIGENCE_SCHEDULER_STATS_INTERVAL_SEC=180
scheduler_stats_interval_sec = 180

# ============================================================================
# Weighted Fair Queuing (WFQ) Configuration
# ============================================================================
# WFQ weights for each priority level (comma-separated)
# Higher weight = more bandwidth share in WFQ scheduling
#
# Default exponential weights for 10 levels:
#   P0: 256 (2^8) - Gets 256x more bandwidth than P9
#   P1: 128 (2^7)
#   P2: 64  (2^6)
#   P3: 32  (2^5)
#   P4: 16  (2^4)
#   P5: 8   (2^3)
#   P6: 4   (2^2)
#   P7: 2   (2^1)
#   P8: 1   (2^0)
#   P9: 1   (2^0) - Lowest priority
#
# Example custom weights (linear instead of exponential):
#   scheduler_weights = 10,9,8,7,6,5,4,3,2,1
#
# Example equal weights (pure round-robin, ignore priority):
#   scheduler_weights = 1,1,1,1,1,1,1,1,1,1
#
# IMPORTANT: Number of weights MUST match scheduler_priority_levels
#
# Override: TOKLIGENCE_SCHEDULER_WEIGHTS="256,128,64,32,16,8,4,2,1,1"
# scheduler_weights = 256,128,64,32,16,8,4,2,1,1

# ============================================================================
# Monitoring & Observability
# ============================================================================
# Scheduler exposes detailed debug logs at DEBUG level
# Set log_level=debug in gateway.ini to see:
#   - Queue enqueue/dequeue events with timestamps
#   - Capacity checks and utilization metrics
#   - WFQ deficit tracking and scheduling decisions
#   - Request wait times and queue positions
#
# Prometheus metrics (future):
#   tokligence_scheduler_queue_depth{priority="0"} 5
#   tokligence_scheduler_wait_p99_ms{priority="5"} 500
#   tokligence_scheduler_capacity_util{dimension="concurrent"} 0.75

# ============================================================================
# Performance Tuning Guidelines
# ============================================================================
# 1. Start with defaults (10 levels, hybrid policy, 10k tokens/sec)
# 2. Monitor queue depths per priority level for 1 week
# 3. Adjust based on observations:
#
#    Most queues empty?
#      → Reduce scheduler_priority_levels to 5
#
#    Many requests in same queue?
#      → Increase scheduler_priority_levels to 15-20
#
#    P0 queue often full?
#      → Increase scheduler_max_queue_depth for P0 only (future)
#
#    Low priority starving?
#      → Switch from "strict" to "hybrid" policy
#      → Or adjust weights to be less exponential
#
#    High latency under load?
#      → Increase scheduler_max_tokens_per_sec (if cluster capacity allows)
#      → Or reduce scheduler_max_concurrent to avoid context switching
#
# 4. Benchmark regularly: tokligence benchmark --profile long_context

# ============================================================================
# Example Use Cases
# ============================================================================
# Example 1: Simple 3-tier system (High/Normal/Low)
#   scheduler_priority_levels = 3
#   scheduler_default_priority = 1  # P1 = Normal
#   scheduler_weights = 10,5,1
#
# Example 2: Enterprise multi-tenant (Gold/Silver/Bronze customers)
#   scheduler_priority_levels = 5
#   # P0=Internal, P1=Gold, P2=Silver, P3=Bronze, P4=Batch
#   scheduler_weights = 100,50,25,10,1
#
# Example 3: Environment isolation (Prod/Staging/Dev)
#   scheduler_priority_levels = 10
#   # P0-P2=Production, P3-P5=Staging, P6-P9=Dev
#   scheduler_policy = hybrid
#
# Example 4: High-concurrency cluster (many small requests)
#   scheduler_max_concurrent = 500
#   scheduler_max_rps = 1000
#   scheduler_max_tokens_per_sec = 50000
